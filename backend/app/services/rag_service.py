"""
LexFlow Protocol - RAG (Retrieval-Augmented Generation) ã‚µãƒ¼ãƒ“ã‚¹
å¥‘ç´„æ›¸ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€æ¤œç´¢ã€ãŠã‚ˆã³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚’æ‹…å½“
"""
import os
import chromadb
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List, Dict, Any, Optional

from app.core.config import settings

class RAGService:
    """
    RAGã‚µãƒ¼ãƒ“ã‚¹
    ChromaDBã‚’ä½¿ç”¨ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã¨æ¤œç´¢ã‚’è¡Œã†
    """
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=settings.OPENAI_API_KEY
        )
        
        # æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ãƒ‘ã‚¹è¨­å®š
        self.persist_directory = os.path.join(os.getcwd(), "chroma_db")
        os.makedirs(self.persist_directory, exist_ok=True)
        
        # ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        
        print(f"ğŸ“¦ RAG ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–: {self.persist_directory}")

    def _get_vectorstore(self, workspace_id: str):
        """
        ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã”ã¨ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å–å¾—
        ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã¯ workspace_id ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹
        """
        collection_name = f"workspace_{workspace_id.replace('-', '_')}"
        
        return Chroma(
            client=self.client,
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )

    async def index_contract(self, contract_id: str, workspace_id: str, text: str, metadata: Dict[str, Any] = None):
        """
        å¥‘ç´„æ›¸ã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã«ç™»éŒ²
        """
        if not text or len(text.strip()) < 10:
            print(f"âš ï¸ {contract_id}: ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™")
            return
            
        print(f"ğŸ” {contract_id}: ãƒ™ã‚¯ãƒˆãƒ«DBã«ç™»éŒ²ä¸­...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len,
            is_separator_regex=False,
        )
        
        chunks = text_splitter.split_text(text)
        print(f"âœ‚ï¸ {contract_id}: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        final_metadata = {
            "contract_id": contract_id,
            "workspace_id": workspace_id,
        }
        if metadata:
            final_metadata.update(metadata)
            
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å–å¾—
        vectorstore = self._get_vectorstore(workspace_id)
        
        # æ—¢å­˜ã®å½“è©²ã‚³ãƒ³ãƒˆãƒ©ã‚¯ãƒˆæƒ…å ±ã‚’å‰Šé™¤ï¼ˆå†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨ï¼‰
        try:
            vectorstore.delete(where={"contract_id": contract_id})
        except Exception:
            pass # æœªç™»éŒ²ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŒç„¡è¦–
            
        # ç™»éŒ²
        vectorstore.add_texts(
            texts=chunks,
            metadatas=[final_metadata] * len(chunks)
        )
        
        print(f"âœ… {contract_id}: ãƒ™ã‚¯ãƒˆãƒ«DBã«ç™»éŒ²ã—ã¾ã—ãŸ")

    async def search_relevant_context(self, workspace_id: str, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
        """
        vectorstore = self._get_vectorstore(workspace_id)
        
        results = vectorstore.similarity_search_with_score(query, k=limit)
        
        formatted_results = []
        for doc, score in results:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": float(score)
            })
            
        return formatted_results

    async def query_with_context(self, workspace_id: str, query: str, limit: int = 5) -> Dict[str, Any]:
        """
        RAGæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€OpenAI APIã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆ
        
        Returns:
            answer: AIã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
            sources: å¼•ç”¨å…ƒã®å¥‘ç´„æ›¸æƒ…å ±ã¨ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®ãƒªã‚¹ãƒˆ
        """
        from openai import AsyncOpenAI
        
        # é–¢é€£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œç´¢
        contexts = await self.search_relevant_context(workspace_id, query, limit=limit)
        
        if not contexts:
            return {
                "answer": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€é–¢é€£ã™ã‚‹å¥‘ç´„æ›¸ã®æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®è¡¨ç¾ã§è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
                "sources": []
            }
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        context_texts = []
        for idx, ctx in enumerate(contexts, 1):
            title = ctx["metadata"].get("title", "ä¸æ˜ãªå¥‘ç´„æ›¸")
            content = ctx["content"]
            context_texts.append(f"ã€å¥‘ç´„æ›¸ {idx}: {title}ã€‘\n{content}")
        
        combined_context = "\n\n".join(context_texts)
        
        # OpenAI APIã‚’å‘¼ã³å‡ºã—
        client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        
        system_prompt = """ã‚ãªãŸã¯å¥‘ç´„æ›¸ã®å°‚é–€å®¶ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å¥‘ç´„æ›¸ã®æŠœç²‹ã‚’å‚ç…§ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
- å¿…ãšæä¾›ã•ã‚ŒãŸå¥‘ç´„æ›¸ã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
- å›ç­”ã®æ ¹æ‹ ã¨ãªã‚‹å¥‘ç´„æ›¸åã‚„æ¡é …ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- ä¸ç¢ºå®Ÿãªå ´åˆã‚„å¥‘ç´„æ›¸ã«è¨˜è¼‰ãŒãªã„å ´åˆã¯ã€ã€Œå¥‘ç´„æ›¸ã«ã¯æ˜è¨˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„
- ç®‡æ¡æ›¸ãã‚’ä½¿ã£ã¦æ•´ç†ã•ã‚ŒãŸå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„"""

        user_prompt = f"""å¥‘ç´„æ›¸ã®æŠœç²‹:
{combined_context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}"""

        try:
            response = await client.chat.completions.create(
                model="gpt-4o-mini",  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # ä¸€è²«æ€§ã®ã‚ã‚‹å›ç­”ã®ãŸã‚ä½ã‚ã«è¨­å®š
                max_tokens=800
            )
            
            answer = response.choices[0].message.content
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ•´å½¢
            sources = []
            for ctx in contexts:
                sources.append({
                    "contract_id": ctx["metadata"].get("contract_id"),
                    "title": ctx["metadata"].get("title", "ä¸æ˜ãªå¥‘ç´„æ›¸"),
                    "excerpt": ctx["content"][:200] + "..." if len(ctx["content"]) > 200 else ctx["content"],
                    "relevance_score": 1.0 / (1.0 + ctx["score"])  # ã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
                })
            
            return {
                "answer": answer,
                "sources": sources
            }
            
        except Exception as e:
            print(f"âŒ OpenAI API Error: {e}")
            return {
                "answer": f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "sources": [
                    {
                        "contract_id": ctx["metadata"].get("contract_id"),
                        "title": ctx["metadata"].get("title", "ä¸æ˜ãªå¥‘ç´„æ›¸"),
                        "excerpt": ctx["content"][:200] + "...",
                        "relevance_score": 1.0 / (1.0 + ctx["score"])
                    }
                    for ctx in contexts
                ]
            }

# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rag_service = RAGService()
